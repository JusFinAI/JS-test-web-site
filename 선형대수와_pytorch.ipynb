{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JusFinAI/JS-test-web-site/blob/main/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%EC%99%80_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyGhi8aw28AK"
      },
      "source": [
        "# 선형대수와 PyTorch: 수학적 직관으로 이해하는 딥러닝의 기초\n",
        "\n",
        "## 학습 목표\n",
        "1. 행렬 곱셈의 수학적 의미를 이해하고 이를 PyTorch로 구현할 수 있다\n",
        "2. 차원 변환의 관점에서 행렬 곱셈을 이해할 수 있다\n",
        "3. PyTorch의 행렬 연산 방식과 그 이유를 이해할 수 있다\n",
        "4. 실제 딥러닝에서 사용되는 행렬 연산의 기초를 이해할 수 있다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4al81po28AM"
      },
      "source": [
        "### 1. 시작하기: 필요한 도구 준비하기\n",
        "PyTorch를 사용하기 위해 필요한 기본적인 설정을 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcbS5j6m28AM",
        "outputId": "ded4728e-bd3c-478f-fcd6-fa79cbdeb014"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79b972bd7d70>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 재현성을 위한 시드 설정\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ciG17S628AN"
      },
      "source": [
        "### 2. 수학에서의 행렬 곱셈 복습\n",
        "행렬 곱셈의 가장 기본적인 원리를 복습해봅시다.  \n",
        "\n",
        "(m × n) 행렬과 (n × 1) 벡터를 곱하면 (m × 1) 벡터가 나옵니다. 이는 우리가 알고 있는 가장 기본적인 행렬 곱셈의 형태입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9jVUsd228AN",
        "outputId": "34bfc4b8-d566-4e85-8051-e99c73011411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W의 shape: torch.Size([2, 3])\n",
            "x의 shape: torch.Size([3, 1])\n",
            "\n",
            "행렬 곱의 결과:\n",
            "y의 shape: torch.Size([2, 1])\n",
            "tensor([[14],\n",
            "        [32]])\n"
          ]
        }
      ],
      "source": [
        "# 2x3 행렬과 3x1 벡터의 곱\n",
        "W = torch.tensor([[1, 2, 3],\n",
        "                [4, 5, 6]])  # 2x3 행렬\n",
        "x = torch.tensor([[1],\n",
        "                [2],\n",
        "                [3]])        # 3x1 벡터\n",
        "\n",
        "print(\"W의 shape:\", W.shape)  # [2, 3]\n",
        "print(\"x의 shape:\", x.shape)  # [3, 1]\n",
        "\n",
        "# 행렬 곱셈\n",
        "y = torch.matmul(W, x)\n",
        "print(\"\\n행렬 곱의 결과:\")\n",
        "print(f\"y의 shape: {y.shape}\")  # [2, 1]\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpVmlCa928AO"
      },
      "source": [
        "### 3. 차원 변환으로 이해하는 행렬 곱셈\n",
        "행렬 곱셈을 '한 차원에서 다른 차원으로의 변환'으로 이해해봅시다. W의 shape가 [출력차원, 입력차원]이라는 점에 주목해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shqKf6tG28AO",
        "outputId": "3c6422d1-2f2c-4cfa-e7da-f8f4002824b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "변환 전 차원: torch.Size([4, 1])\n",
            "변환 후 차원: torch.Size([2, 1])\n",
            "\n",
            "가중치 행렬 W의 shape: torch.Size([2, 4])\n",
            "이는 [출력차원, 입력차원]의 형태입니다!\n"
          ]
        }
      ],
      "source": [
        "# 입력 차원과 출력 차원 정의\n",
        "input_dim = 4   # 입력 공간의 차원\n",
        "output_dim = 2  # 출력 공간의 차원\n",
        "\n",
        "# 변환을 위한 가중치 행렬\n",
        "W = torch.randn(output_dim, input_dim)  # [2, 4]\n",
        "\n",
        "# 입력 벡터 (4차원 공간의 한 점)\n",
        "x = torch.randn(input_dim, 1)  # [4, 1]\n",
        "\n",
        "# 차원 변환\n",
        "y = torch.matmul(W, x)  # [2, 1]\n",
        "\n",
        "print(\"변환 전 차원:\", x.shape)\n",
        "print(\"변환 후 차원:\", y.shape)\n",
        "print(\"\\n가중치 행렬 W의 shape:\", W.shape)\n",
        "print(\"이는 [출력차원, 입력차원]의 형태입니다!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogVqZ4eG28AO"
      },
      "source": [
        "### 4. PyTorch의 행렬 연산 방식 이해하기\n",
        "PyTorch는 행렬 곱셈을 조금 다르게 표현합니다. 수학에서는 y = Wx + b 형태였지만, PyTorch에서는 y = xW^T + b 형태를 사용합니다. 이러한 차이가 생긴 이유를 알아봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHMKC02E28AP",
        "outputId": "bca66209-061a-40e8-8e81-f3bc86960c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "수학 방식 결과:\n",
            "tensor([[14],\n",
            "        [32]])\n",
            "\n",
            "PyTorch 방식 결과:\n",
            "tensor([[14],\n",
            "        [32]])\n"
          ]
        }
      ],
      "source": [
        "# 전통적인 수학 방식\n",
        "x_math = torch.tensor([[1],\n",
        "                    [2],\n",
        "                    [3]])        # 3x1 벡터\n",
        "\n",
        "# PyTorch 방식\n",
        "x_torch = torch.tensor([[1, 2, 3]])  # 1x3 벡터 (행벡터)\n",
        "\n",
        "# 동일한 가중치 행렬\n",
        "W = torch.tensor([[1, 2, 3],\n",
        "                [4, 5, 6]])\n",
        "\n",
        "# 두 가지 방식의 결과 비교\n",
        "y_math = torch.matmul(W, x_math)\n",
        "y_torch = torch.matmul(x_torch, W.T)  # W.T는 W의 전치(transpose)\n",
        "\n",
        "print(\"수학 방식 결과:\")\n",
        "print(y_math)\n",
        "print(\"\\nPyTorch 방식 결과:\")\n",
        "print(y_torch.T)  # 비교를 위해 전치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH0jQKkF28AP"
      },
      "source": [
        "### 5. 배치 처리: PyTorch 방식의 장점\n",
        "PyTorch가 행벡터를 사용하는 이유는 여러 데이터를 한번에 처리하기 위해서입니다. 이것을 '배치 처리'라고 합니다. 여러 데이터를 동시에 처리하면 컴퓨터의 자원을 효율적으로 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di-60eN728AP",
        "outputId": "e1b60863-a786-40d0-f0b7-5af2e06da8f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 데이터 shape: torch.Size([3, 3])\n",
            "가중치 행렬 shape: torch.Size([2, 3])\n",
            "출력 결과 shape: torch.Size([3, 2])\n",
            "\n",
            "각 데이터의 결과:\n",
            "\n",
            "데이터 1의 결과: tensor([14, 32])\n",
            "\n",
            "데이터 2의 결과: tensor([32, 77])\n",
            "\n",
            "데이터 3의 결과: tensor([ 50, 122])\n"
          ]
        }
      ],
      "source": [
        "# 여러 데이터를 한번에 처리하기\n",
        "batch_size = 3\n",
        "x_batch = torch.tensor([[1, 2, 3],   # 첫 번째 데이터\n",
        "                    [4, 5, 6],    # 두 번째 데이터\n",
        "                    [7, 8, 9]])   # 세 번째 데이터\n",
        "\n",
        "W = torch.tensor([[1, 2, 3],\n",
        "                [4, 5, 6]])\n",
        "\n",
        "# 한 번의 연산으로 세 개의 데이터를 처리\n",
        "y_batch = torch.matmul(x_batch, W.T)\n",
        "\n",
        "print(\"입력 데이터 shape:\", x_batch.shape)  # [3, 3]\n",
        "print(\"가중치 행렬 shape:\", W.shape)        # [2, 3]\n",
        "print(\"출력 결과 shape:\", y_batch.shape)    # [3, 2]\n",
        "\n",
        "print(\"\\n각 데이터의 결과:\")\n",
        "for i in range(batch_size):\n",
        "    print(f\"\\n데이터 {i+1}의 결과:\", y_batch[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX70ceio28AP"
      },
      "source": [
        "### 6. PyTorch의 Linear 층 사용하기\n",
        "실제 딥러닝에서는 이러한 행렬 곱셈을 Linear 층을 통해 쉽게 사용할 수 있습니다. Linear 층은 우리가 배운 행렬 곱셈을 편리하게 사용할 수 있도록 만든 도구입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEHVHAzK28AQ",
        "outputId": "0f3d49f7-1056-4aee-f703-ddca3767ac47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear 층의 결과:\n",
            "tensor([[ 14.,  32.],\n",
            "        [ 32.,  77.],\n",
            "        [ 50., 122.]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Linear 층의 가중치 확인:\n",
            "Parameter containing:\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Linear 층 생성\n",
        "input_features = 3\n",
        "output_features = 2\n",
        "linear = nn.Linear(input_features, output_features)\n",
        "\n",
        "# 앞서 사용한 가중치로 설정\n",
        "with torch.no_grad():\n",
        "    linear.weight = nn.Parameter(torch.tensor([[1., 2., 3.],\n",
        "                                            [4., 5., 6.]]))\n",
        "    linear.bias = nn.Parameter(torch.zeros(2))\n",
        "\n",
        "# 배치 데이터로 테스트\n",
        "x_batch = torch.tensor([[1, 2, 3],\n",
        "                    [4, 5, 6],\n",
        "                    [7, 8, 9.]], dtype=torch.float32)\n",
        "\n",
        "output = linear(x_batch)\n",
        "print(\"Linear 층의 결과:\")\n",
        "print(output)\n",
        "\n",
        "print(\"\\nLinear 층의 가중치 확인:\")\n",
        "print(linear.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmGtQVZt28AQ"
      },
      "source": [
        "### 7. 실제 적용: RNN에서의 행렬 곱셈\n",
        "이제 우리가 배운 행렬 곱셈이 실제 딥러닝 모델에서 어떻게 사용되는지 RNN을 예로 들어 살펴봅시다. RNN은 시퀀스 데이터를 처리하는 신경망으로, 각 시점에서 입력을 처리할 때 우리가 배운 행렬 곱셈이 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwU1TPzz28AQ",
        "outputId": "51ed2546-feff-46cc-eb1e-ba7f5198e463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "한 시점 입력의 shape: torch.Size([2, 4])\n",
            "가중치 행렬의 shape: torch.Size([3, 4])\n",
            "변환 결과의 shape: torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "# RNN의 입력 처리 예시\n",
        "input_dim = 4    # 입력 특성의 차원\n",
        "hidden_dim = 3   # 은닉 상태의 차원\n",
        "batch_size = 2   # 배치 크기\n",
        "sequence_len = 5 # 시퀀스 길이\n",
        "\n",
        "# 입력 데이터 생성\n",
        "x = torch.randn(batch_size, sequence_len, input_dim)\n",
        "\n",
        "# 가중치 행렬 (입력을 은닉 차원으로 변환)\n",
        "W = torch.randn(hidden_dim, input_dim)\n",
        "\n",
        "# 첫 번째 시점의 입력만 처리해보기\n",
        "x_t = x[:, 0, :]  # 첫 번째 시점의 입력\n",
        "y_t = torch.matmul(x_t, W.T)\n",
        "\n",
        "print(\"한 시점 입력의 shape:\", x_t.shape)      # [batch_size, input_dim]\n",
        "print(\"가중치 행렬의 shape:\", W.shape)         # [hidden_dim, input_dim]\n",
        "print(\"변환 결과의 shape:\", y_t.shape)         # [batch_size, hidden_dim]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxeCBUUu28AQ"
      },
      "source": [
        "### 정리\n",
        "지금까지 우리는:\n",
        "1. 행렬 곱셈의 수학적 의미\n",
        "2. 차원 변환으로서의 행렬 곱셈\n",
        "3. PyTorch의 행렬 연산 방식과 그 이점\n",
        "4. 실제 딥러닝에서의 적용\n",
        "\n",
        "을 살펴보았습니다. 특히 중요한 점은 행렬 곱셈이 \"한 차원에서 다른 차원으로의 변환\"이라는 것과, PyTorch가 배치 처리의 효율성을 위해 행벡터 방식을 선택했다는 것입니다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}